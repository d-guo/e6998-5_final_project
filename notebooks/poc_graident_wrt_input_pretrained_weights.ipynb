{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e1cc1d",
   "metadata": {},
   "source": [
    "references\n",
    "1. https://github.com/ultralytics/ultralytics/issues/1693\n",
    "2. https://discuss.pytorch.org/t/what-is-the-purpose-of-is-leaf/87000/3\n",
    "3. https://pytorch.org/docs/master/autograd.html?highlight=grad#torch.autograd.grad\n",
    "4. https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854402d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.yolo.v8.detect.train import DetectionTrainer\n",
    "from ultralytics.yolo.data.dataloaders.v5loader import create_dataloader\n",
    "from ultralytics.yolo.v8.detect.train import Loss\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import resize_right\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4db6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.86 ðŸš€ Python-3.9.16 torch-2.0.0 CPU\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=predict, model=../model_results/train/weights/best.pt, data=../detection_data.yaml, epochs=1, patience=50, batch=1, imgsz=128, save=False, save_period=-1, cache=False, device=None, workers=4, project=None, name=None, exist_ok=False, pretrained=False, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=1, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.4, degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=.\n"
     ]
    }
   ],
   "source": [
    "# load detection trainer using the weights best.pt\n",
    "# best.pt is the custom model trained on YOLO_data_je\n",
    "trainer = DetectionTrainer('args.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a3c3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.Detect                [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': -1,\n",
       " 'best_fitness': None,\n",
       " 'model': DetectionModel(\n",
       "   (model): Sequential(\n",
       "     (0): Conv(\n",
       "       (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (1): Conv(\n",
       "       (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (2): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0): Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (3): Conv(\n",
       "       (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (4): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0-1): 2 x Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (5): Conv(\n",
       "       (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (6): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0-1): 2 x Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (7): Conv(\n",
       "       (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (8): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0): Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (9): SPPF(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "     (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "     (11): Concat()\n",
       "     (12): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0): Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "     (14): Concat()\n",
       "     (15): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0): Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (16): Conv(\n",
       "       (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (17): Concat()\n",
       "     (18): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0): Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (19): Conv(\n",
       "       (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "       (act): SiLU(inplace=True)\n",
       "     )\n",
       "     (20): Concat()\n",
       "     (21): C2f(\n",
       "       (cv1): Conv(\n",
       "         (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (cv2): Conv(\n",
       "         (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "         (act): SiLU(inplace=True)\n",
       "       )\n",
       "       (m): ModuleList(\n",
       "         (0): Bottleneck(\n",
       "           (cv1): Conv(\n",
       "             (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (cv2): Conv(\n",
       "             (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (22): Detect(\n",
       "       (cv2): ModuleList(\n",
       "         (0): Sequential(\n",
       "           (0): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (1): Sequential(\n",
       "           (0): Conv(\n",
       "             (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (2): Sequential(\n",
       "           (0): Conv(\n",
       "             (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "       )\n",
       "       (cv3): ModuleList(\n",
       "         (0): Sequential(\n",
       "           (0): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (2): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (1): Sequential(\n",
       "           (0): Conv(\n",
       "             (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (2): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (2): Sequential(\n",
       "           (0): Conv(\n",
       "             (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (1): Conv(\n",
       "             (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "             (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "             (act): SiLU(inplace=True)\n",
       "           )\n",
       "           (2): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "       )\n",
       "       (dfl): DFL(\n",
       "         (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'ema': None,\n",
       " 'updates': None,\n",
       " 'optimizer': None,\n",
       " 'train_args': {'task': 'detect',\n",
       "  'mode': 'train',\n",
       "  'model': 'yolov8n.pt',\n",
       "  'data': '/burg/home/gj2353/model_run/detection_data.yaml',\n",
       "  'epochs': 30,\n",
       "  'patience': 50,\n",
       "  'batch': 16,\n",
       "  'imgsz': 640,\n",
       "  'save': True,\n",
       "  'save_period': -1,\n",
       "  'cache': False,\n",
       "  'device': None,\n",
       "  'workers': 4,\n",
       "  'project': None,\n",
       "  'name': None,\n",
       "  'exist_ok': False,\n",
       "  'pretrained': False,\n",
       "  'optimizer': 'Adam',\n",
       "  'verbose': True,\n",
       "  'seed': 0,\n",
       "  'deterministic': True,\n",
       "  'single_cls': False,\n",
       "  'image_weights': False,\n",
       "  'rect': False,\n",
       "  'cos_lr': False,\n",
       "  'close_mosaic': 0,\n",
       "  'resume': False,\n",
       "  'amp': True,\n",
       "  'overlap_mask': True,\n",
       "  'mask_ratio': 4,\n",
       "  'dropout': 0.0,\n",
       "  'val': True,\n",
       "  'split': 'val',\n",
       "  'save_json': False,\n",
       "  'save_hybrid': False,\n",
       "  'conf': None,\n",
       "  'iou': 0.7,\n",
       "  'max_det': 300,\n",
       "  'half': False,\n",
       "  'dnn': False,\n",
       "  'plots': True,\n",
       "  'source': None,\n",
       "  'show': False,\n",
       "  'save_txt': False,\n",
       "  'save_conf': False,\n",
       "  'save_crop': False,\n",
       "  'hide_labels': False,\n",
       "  'hide_conf': False,\n",
       "  'vid_stride': 1,\n",
       "  'line_thickness': 3,\n",
       "  'visualize': False,\n",
       "  'augment': False,\n",
       "  'agnostic_nms': False,\n",
       "  'classes': None,\n",
       "  'retina_masks': False,\n",
       "  'boxes': True,\n",
       "  'format': 'torchscript',\n",
       "  'keras': False,\n",
       "  'optimize': False,\n",
       "  'int8': False,\n",
       "  'dynamic': False,\n",
       "  'simplify': False,\n",
       "  'opset': None,\n",
       "  'workspace': 4,\n",
       "  'nms': False,\n",
       "  'lr0': 0.001,\n",
       "  'lrf': 0.01,\n",
       "  'momentum': 0.937,\n",
       "  'weight_decay': 0.0005,\n",
       "  'warmup_epochs': 3.0,\n",
       "  'warmup_momentum': 0.8,\n",
       "  'warmup_bias_lr': 0.1,\n",
       "  'box': 7.5,\n",
       "  'cls': 0.5,\n",
       "  'dfl': 1.5,\n",
       "  'fl_gamma': 0.0,\n",
       "  'label_smoothing': 0.0,\n",
       "  'nbs': 64,\n",
       "  'hsv_h': 0.0,\n",
       "  'hsv_s': 0.0,\n",
       "  'hsv_v': 0.4,\n",
       "  'degrees': 0.0,\n",
       "  'translate': 0.0,\n",
       "  'scale': 0.0,\n",
       "  'shear': 0.0,\n",
       "  'perspective': 0.0,\n",
       "  'flipud': 0.0,\n",
       "  'fliplr': 0.5,\n",
       "  'mosaic': 1.0,\n",
       "  'mixup': 0.0,\n",
       "  'copy_paste': 0.0,\n",
       "  'cfg': None,\n",
       "  'v5loader': False,\n",
       "  'tracker': 'botsort.yaml'},\n",
       " 'date': '2023-04-21T23:40:10.528823',\n",
       " 'version': '8.0.58'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc8549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_model_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb73b40",
   "metadata": {},
   "source": [
    "## Version using image resizing (without dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64961124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 112, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "org_img = cv2.imread('../data/YOLO_short/images/0.png')\n",
    "print(org_img.shape)\n",
    "plt.imshow(org_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f670e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 28, 112)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so that the channel comes first (xdim, ydim, channel) -> (channel, xdim, ydim); threw an error when order different\n",
    "# error: expected input[1, 128, 128, 3] to have 3 channels, but got 128 channels instead\n",
    "input_img = np.moveaxis(org_img, -1, 0)\n",
    "input_img.shape\n",
    "\n",
    "input_img = np.expand_dims(input_img, axis=0) # to add (1, )\n",
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0027a9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 28, 112])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_img = torch.tensor(np.array(input_img, dtype='float32'), requires_grad=True)\n",
    "tensor_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d78356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_img_resized = resize_right.resize(tensor_img, out_shape=(1,3,128,128))\n",
    "tensor_img_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6d8403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definining batch with true label and bboxes to calculate the loss\n",
    "\n",
    "batch = {'ori_shape': ((28, 112),),\n",
    " 'ratio_pad': None,\n",
    " 'im_file': None,\n",
    " 'img': None,\n",
    " 'cls': torch.tensor([[8.],\n",
    "         [6.],\n",
    "         [0.],\n",
    "         [5.]]),\n",
    " 'bboxes': torch.tensor([[0.1250, 0.5000, 0.2250, 0.2250],\n",
    "         [0.3750, 0.5000, 0.2250, 0.2250],\n",
    "         [0.6250, 0.5000, 0.2250, 0.2250],\n",
    "         [0.8750, 0.5000, 0.2250, 0.2250]]),\n",
    " 'batch_idx': torch.tensor([0., 0., 0., 0.])}\n",
    "\n",
    "# resized image we feed to the model\n",
    "batch['img'] = tensor_img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7acae752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where backpropagation happens\n",
    "a = trainer.model(tensor_img_resized)\n",
    "loss_fn = Loss(trainer.model)\n",
    "loss, _ = loss_fn(a, batch)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9818695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 112])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9.2492e-04,  1.5137e-03,  1.3247e-03,  ..., -5.8940e-05, -2.7097e-04, -2.1566e-04],\n",
       "          [ 3.5895e-03,  3.8699e-03,  2.9743e-03,  ...,  3.5412e-04,  1.8898e-04,  1.2386e-04],\n",
       "          [ 1.3349e-03,  1.1913e-03,  1.0731e-03,  ..., -3.5590e-04, -4.4534e-04, -3.2851e-04],\n",
       "          ...,\n",
       "          [-1.8550e-03, -1.3704e-03, -3.8579e-04,  ..., -2.0221e-03, -3.2223e-03, -2.2100e-03],\n",
       "          [-1.5323e-03, -4.9775e-04,  5.4932e-04,  ..., -2.3425e-04, -1.8850e-03, -1.5106e-03],\n",
       "          [ 1.2628e-03,  1.5794e-03,  1.5015e-03,  ...,  1.8404e-03,  9.0323e-04,  5.8194e-04]],\n",
       "\n",
       "         [[-1.7921e-03, -7.1454e-04, -2.1065e-04,  ...,  3.4854e-05,  1.1420e-04,  1.2718e-04],\n",
       "          [-5.8348e-03, -3.2799e-03, -1.5298e-03,  ..., -1.8148e-04, -9.5691e-05, -3.4160e-05],\n",
       "          [-2.0760e-03, -1.3825e-03, -8.0358e-04,  ..., -4.6732e-05,  1.8254e-04,  1.5429e-04],\n",
       "          ...,\n",
       "          [ 2.6248e-03,  1.3782e-03,  2.4971e-04,  ...,  5.7348e-04,  6.3507e-04,  1.0064e-03],\n",
       "          [ 1.7433e-03,  9.3811e-04,  9.3101e-05,  ...,  1.4202e-04,  6.9108e-04,  8.5908e-04],\n",
       "          [-1.4938e-03, -1.0829e-03, -7.4267e-04,  ..., -1.3060e-03, -7.0621e-04, -2.4914e-04]],\n",
       "\n",
       "         [[ 1.9196e-04, -6.4956e-04, -1.1350e-03,  ..., -1.0411e-04,  1.4046e-04,  1.7340e-04],\n",
       "          [ 7.1852e-04, -6.9208e-04, -1.5998e-03,  ..., -1.9688e-04, -1.6772e-04, -8.6245e-05],\n",
       "          [ 5.3774e-04,  6.7465e-05, -1.6880e-04,  ...,  3.3452e-04,  3.3008e-04,  2.3291e-04],\n",
       "          ...,\n",
       "          [-1.1216e-04, -5.0138e-06,  2.9307e-05,  ...,  1.3726e-03,  2.1379e-03,  1.8124e-03],\n",
       "          [ 2.4394e-04, -1.6866e-04, -6.1732e-04,  ...,  2.1353e-04,  1.2775e-03,  1.2784e-03],\n",
       "          [-5.8660e-05, -5.9258e-04, -9.8245e-04,  ..., -1.0142e-03, -7.0136e-04, -3.4959e-04]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tensor_img.grad.shape)\n",
    "tensor_img.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70acd792",
   "metadata": {},
   "source": [
    "## one using loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc72538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning ../data/YOLO_short/labels.cache... 6 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = create_dataloader('../data/YOLO_short/images', imgsz=128,\n",
    "                           batch_size=1,\n",
    "                           stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "542b8759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(loader[0]))\n",
    "batch['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60315e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check if the batch['img'] is a leaf node; if it is .backward() accumulates grad\n",
    "batch['img']=batch['img'].float()\n",
    "print(batch['img'].is_leaf)\n",
    "print(batch['img'].requires_grad)\n",
    "batch['img'].requires_grad=True # additional argument to get the gradient of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0025373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2903270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ori_shape': ((28, 112),),\n",
       " 'ratio_pad': (((1.1428571428571428, 1.1428571428571428), (0.0, 48.0)),),\n",
       " 'im_file': ('../data/YOLO_short/images/0.png',),\n",
       " 'img': tensor([[[[114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           ...,\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.]],\n",
       " \n",
       "          [[114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           ...,\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.]],\n",
       " \n",
       "          [[114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           ...,\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.],\n",
       "           [114., 114., 114.,  ..., 114., 114., 114.]]]], requires_grad=True),\n",
       " 'cls': tensor([[8.],\n",
       "         [6.],\n",
       "         [0.],\n",
       "         [5.]]),\n",
       " 'bboxes': tensor([[0.1250, 0.5000, 0.2250, 0.2250],\n",
       "         [0.3750, 0.5000, 0.2250, 0.2250],\n",
       "         [0.6250, 0.5000, 0.2250, 0.2250],\n",
       "         [0.8750, 0.5000, 0.2250, 0.2250]]),\n",
       " 'batch_idx': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch variable set from the loader has cls (true label of the images)\n",
    "# it reads the data from data=../detection_data.yaml, in args.yaml when calling the trainer\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "657c2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where backpropagation happens\n",
    "a = trainer.model(batch['img'])\n",
    "loss_fn = Loss(trainer.model)\n",
    "loss, _ = loss_fn(a, batch)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2f669a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch['img'].grad).shape # this version gives the gradient w.r.t. resized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a80fcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.0678e-05, -1.8723e-04, -5.6495e-04,  ...,  5.5701e-03,  7.3125e-03,  5.1939e-03],\n",
       "          [ 8.4913e-04,  4.8150e-05, -1.2031e-03,  ...,  6.1341e-03,  8.8653e-03,  5.9796e-03],\n",
       "          [ 2.5488e-03,  7.3475e-04, -2.4057e-03,  ...,  5.9311e-03,  5.5987e-03,  3.4876e-03],\n",
       "          ...,\n",
       "          [-2.4228e-03, -9.3500e-04, -3.8466e-04,  ..., -5.1462e-03, -6.4833e-03, -4.9381e-03],\n",
       "          [-4.3758e-03, -4.8144e-03, -3.6602e-03,  ..., -2.9989e-03, -2.8014e-03, -1.9724e-03],\n",
       "          [-3.9081e-03, -4.6390e-03, -3.2471e-03,  ..., -2.5209e-03, -2.4373e-03, -1.9301e-03]],\n",
       "\n",
       "         [[ 1.4024e-03,  8.5304e-04,  8.9985e-05,  ..., -1.8474e-03, -2.1638e-03, -2.6164e-03],\n",
       "          [ 8.8364e-04,  4.5892e-04,  3.4072e-04,  ..., -2.0824e-03, -1.9692e-03, -2.5403e-03],\n",
       "          [-3.0394e-04, -2.3936e-04,  5.9522e-04,  ..., -1.9198e-03, -1.9524e-03, -2.5911e-03],\n",
       "          ...,\n",
       "          [ 1.7990e-03,  1.2644e-03, -4.1133e-05,  ...,  1.7578e-03,  2.2166e-03,  1.9157e-03],\n",
       "          [ 2.5120e-03,  2.6254e-03,  9.9591e-04,  ...,  7.2385e-04,  8.2494e-04,  9.3026e-04],\n",
       "          [ 1.6558e-03,  1.7067e-03,  7.1367e-04,  ...,  7.5370e-04,  6.1481e-04,  7.0629e-04]],\n",
       "\n",
       "         [[-9.7708e-04, -7.0218e-04, -7.9199e-05,  ..., -5.0148e-03, -5.9407e-03, -4.6220e-03],\n",
       "          [-1.4177e-03, -5.3489e-04,  5.2584e-04,  ..., -4.8861e-03, -6.2195e-03, -4.8487e-03],\n",
       "          [-2.5034e-03, -9.1251e-04,  1.9364e-03,  ..., -4.2927e-03, -4.2793e-03, -3.5426e-03],\n",
       "          ...,\n",
       "          [ 6.2783e-04,  3.4341e-04,  2.5428e-04,  ...,  3.4482e-03,  5.0115e-03,  3.8281e-03],\n",
       "          [ 2.7130e-03,  3.5089e-03,  2.8695e-03,  ...,  1.8451e-03,  2.1835e-03,  1.6798e-03],\n",
       "          [ 2.6101e-03,  3.4982e-03,  2.5554e-03,  ...,  1.8177e-03,  1.9132e-03,  1.6158e-03]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['img'].grad # checked that the gradient w.r.t. input is deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5fce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load the model\n",
    "# model = YOLO(\"model_results/train7/weights/best.pt\")\n",
    "# res = model(data)\n",
    "# # model.train(task='detect', data='../detection_data.yaml',\n",
    "# #              epochs=1, batch=1, augment=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
