{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987e5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "import data\n",
    "import networks as net\n",
    "\n",
    "from networks import Generator, Discriminator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'Blues_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474cf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de20219",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {\n",
    "    'run_name': 'GANs_training_small_data_epoch50_alpha10_beta10_thr05_diter1_giter_4',\n",
    "    'description': '',\n",
    "    \n",
    "    'image_size': (28, 28 * 5),\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'num_epochs': 50,\n",
    "    \n",
    "    'learning_rate_discr': 0.01,\n",
    "    'learning_rate_gen': 0.01,\n",
    "    'discr_iters': 1,\n",
    "    'gen_iters': 4,\n",
    "    \n",
    "    'alpha': 10,\n",
    "    'beta': 10,\n",
    "    'threshold': 0.5,\n",
    "}\n",
    "\n",
    "# extract all hyperparameters for ease of use\n",
    "run_name = hp_dict['run_name']\n",
    "M, N = hp_dict['image_size']\n",
    "\n",
    "batch_size = hp_dict['batch_size']\n",
    "num_epochs = hp_dict['num_epochs']\n",
    "\n",
    "learning_rate_discr = hp_dict['learning_rate_discr']\n",
    "learning_rate_gen = hp_dict['learning_rate_gen']\n",
    "discr_iters = hp_dict['discr_iters']\n",
    "gen_iters = hp_dict['gen_iters']\n",
    "\n",
    "alpha = hp_dict['alpha']\n",
    "beta = hp_dict['beta']\n",
    "threshold = hp_dict['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7207ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory for this training session\n",
    "try:\n",
    "    os.mkdir(f'GANs_training/{run_name}')\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(f'GANs_training/{run_name}')\n",
    "    os.mkdir(f'GANs_training/{run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe510dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record hyperparameters\n",
    "hp_dict_str = pprint.pformat(hp_dict)\n",
    "\n",
    "with open(f'GANs_training/{run_name}/hp_dict', 'wb+') as f:\n",
    "    pkl.dump(hp_dict, f)\n",
    "\n",
    "with open(f'GANs_training/{run_name}/hp_dict.txt', 'w+') as f:\n",
    "    f.write(hp_dict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9259f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = data.GANs_Dataset(data_dir='./gans_data/data_dg')\n",
    "dataset_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38384706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset.image_list[0])\n",
    "plt.imshow(dataset.image_list[0][0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfd0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(gen_perturb):\n",
    "    return torch.sum(torch.clip(torch.linalg.norm(gen_perturb.reshape(gen_perturb.shape[0], 3, M * N), dim=(-1, -2)) - threshold, min=0)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e87982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up models\n",
    "discr_model = Discriminator().double()\n",
    "gen_model = Generator().double()\n",
    "\n",
    "discr_model.to(device)\n",
    "gen_model.to(device)\n",
    "\n",
    "# target_model = # YOLO Model\n",
    "\n",
    "# record model architectures\n",
    "with open(f'GANs_training/{run_name}/discriminator_architecture_summary.txt', 'w+') as f:\n",
    "    f.write(str(discr_model))\n",
    "    \n",
    "with open(f'GANs_training/{run_name}/generator_architecture_summary.txt', 'w+') as f:\n",
    "    f.write(str(gen_model))\n",
    "    \n",
    "# record training log\n",
    "with open(f'GANs_training/{run_name}/training_log.txt', 'w+') as f:\n",
    "    f.write(\"Model training log created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff9e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.94 🚀 Python-3.7.5 torch-1.13.1+cu117 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=predict, model=model_results/model_digit_data_ver1_256/weights/best.pt, data=detection_data.yaml, epochs=100, patience=50, batch=64, imgsz=256, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=None, exist_ok=False, pretrained=False, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.Detect                [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.yolo.v8.detect.train import DetectionTrainer\n",
    "from ultralytics.yolo.data.dataloaders.v5loader import create_dataloader\n",
    "from ultralytics.yolo.v8.detect.train import Loss\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import resize_right\n",
    "import numpy as np\n",
    "\n",
    "# load detection trainer using the weights best.pt\n",
    "# best.pt is the custom model trained on YOLO_data_je\n",
    "trainer = DetectionTrainer('args_digits_data_ver1.yaml')\n",
    "trainer.setup_model()\n",
    "trainer.model.double()\n",
    "trainer.set_model_attributes()\n",
    "trainer.model = trainer.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93ef4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install resize-right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ab53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track metrics\n",
    "loss_discr_list = []\n",
    "loss_gen_list = []\n",
    "\n",
    "loss_discr_by_epoch_list = []\n",
    "loss_gen_by_epoch_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe3a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up or refresh optimizers\n",
    "optimizer_discr = torch.optim.Adam(discr_model.parameters(), lr=learning_rate_discr, amsgrad=True)\n",
    "optimizer_gen = torch.optim.Adam(gen_model.parameters(), lr=learning_rate_gen, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef1ba570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 0; batch: 0; discr loss: 1.3711025679425195; gen loss: 54.63401651944015\n",
      "adv loss: -111.09131622314453; gan loss: 0.9562287743575903; hinge loss: 9.45957797527588; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 1; batch: 0; discr loss: 1.7195390288824837; gen loss: -56.93997138376263\n",
      "adv loss: -161.10955810546875; gan loss: 0.27767437518389587; hinge loss: 7.405509134457173; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 2; batch: 0; discr loss: 1.696501158415426; gen loss: -69.62386099523218\n",
      "adv loss: -187.75820922851562; gan loss: 2.94270173844709; hinge loss: 6.324625591168004; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 3; batch: 0; discr loss: 0.896145640760995; gen loss: -98.77274244757038\n",
      "adv loss: -186.49932861328125; gan loss: 2.0486926323817465; hinge loss: 5.196579987478705; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 4; batch: 0; discr loss: 0.5664398580604808; gen loss: -123.67305635883271\n",
      "adv loss: -195.58782958984375; gan loss: 1.0493583221823772; hinge loss: 4.613390507771413; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 5; batch: 0; discr loss: 0.7080194963463748; gen loss: -125.08667559490354\n",
      "adv loss: -201.76800537109375; gan loss: 1.586146517419617; hinge loss: 4.2514567363160625; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 6; batch: 0; discr loss: 0.6014004009566828; gen loss: -107.73880681404225\n",
      "adv loss: -181.70343017578125; gan loss: 2.1274952300409384; hinge loss: 4.4202059628754355; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 7; batch: 0; discr loss: 0.9373622043926246; gen loss: -112.13528630163195\n",
      "adv loss: -181.10397338867188; gan loss: 0.9102512147559498; hinge loss: 4.6907619515284225; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 8; batch: 0; discr loss: 0.8346032699787664; gen loss: -131.56097802165442\n",
      "adv loss: -213.70677185058594; gan loss: 0.9523611717283228; hinge loss: 4.669438859714846; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 9; batch: 0; discr loss: 0.5425535276143679; gen loss: -123.85494226302997\n",
      "adv loss: -219.55126953125; gan loss: 1.187161233204166; hinge loss: 4.815500921226676; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 10; batch: 0; discr loss: 0.5322118534935627; gen loss: -127.75954439137755\n",
      "adv loss: -224.7894744873047; gan loss: 1.9379176939882623; hinge loss: 4.800839076899194; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 11; batch: 0; discr loss: 0.5222367572988387; gen loss: -137.14639766887842\n",
      "adv loss: -231.9549560546875; gan loss: 2.3331072786052203; hinge loss: 5.278578831695297; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 12; batch: 0; discr loss: 0.5459246447933486; gen loss: -124.42512169961661\n",
      "adv loss: -227.8356170654297; gan loss: 2.414370432476398; hinge loss: 5.73392621914047; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 13; batch: 0; discr loss: 0.5559189581851197; gen loss: -127.3702727660509\n",
      "adv loss: -215.8860626220703; gan loss: 0.980623548319085; hinge loss: 5.900809714326483; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 14; batch: 0; discr loss: 0.7326206164760289; gen loss: -109.69745576112774\n",
      "adv loss: -196.5303955078125; gan loss: 1.4026398116116712; hinge loss: 5.668275838230061; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 15; batch: 0; discr loss: 0.5891728726427725; gen loss: -95.93021655047588\n",
      "adv loss: -207.68040466308594; gan loss: 2.962328977593125; hinge loss: 5.467475849320229; (from last batch)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "epoch: 16; batch: 0; discr loss: 0.5743020569701471; gen loss: -109.94603459283174\n",
      "adv loss: -206.70318603515625; gan loss: 2.124285148428522; hinge loss: 5.247305452973405; (from last batch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4460/4055069503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mgen_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mgen_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "discr_model.train()\n",
    "gen_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    discr_loss_epoch, gen_loss_epoch = 0, 0\n",
    "    for batch_id, (images, classes, annotations) in enumerate(dataset_loader):\n",
    "        images = images.double()\n",
    "        classes = classes.double()\n",
    "        annotations = annotations.double()\n",
    "        \n",
    "        \n",
    "        tensor_img = images.repeat(1, 3, 1, 1)\n",
    "        classes = torch.flatten(classes, end_dim=1)\n",
    "        annotations = torch.flatten(annotations, end_dim=1)\n",
    "        \n",
    "        batch_idx = []\n",
    "        keep_indices = []\n",
    "        batch_i = 0\n",
    "        for i, cls in enumerate(classes):\n",
    "            if cls.item() != -1:\n",
    "                keep_indices.append(i)\n",
    "                batch_idx.append(batch_i)\n",
    "                \n",
    "            if (cls.item() == -1 and batch_idx[-1] == batch_i) or (i + 1) % 5 == 0:\n",
    "                batch_i += 1\n",
    "                \n",
    "        classes = classes[keep_indices]\n",
    "        annotations = annotations[keep_indices]\n",
    "        batch_idx = torch.tensor(batch_idx)\n",
    "        \n",
    "        tensor_img = tensor_img.to(device)\n",
    "        classes = classes.to(device)\n",
    "        annotations = annotations.to(device)\n",
    "        batch_idx = batch_idx.to(device)\n",
    "            \n",
    "        discr_loss, gen_loss = 0, 0\n",
    "        \n",
    "        for discr_i in range(discr_iters):\n",
    "            gen_perturb = gen_model(tensor_img)\n",
    "            images_perturb = tensor_img + gen_perturb\n",
    "            \n",
    "            loss_discr = net.compute_discr_loss_minimax(discr_model, tensor_img, images_perturb)\n",
    "            \n",
    "            optimizer_discr.zero_grad()\n",
    "            loss_discr.backward()\n",
    "            optimizer_discr.step()\n",
    "            \n",
    "            discr_loss += loss_discr.item()\n",
    "            discr_loss_epoch += loss_discr.item()\n",
    "\n",
    "        loss_discr_list.append(discr_loss / discr_iters)\n",
    "\n",
    "        for gen_i in range(gen_iters):\n",
    "            gen_perturb = gen_model(tensor_img)\n",
    "            images_perturb = tensor_img + gen_perturb\n",
    "            \n",
    "            tensor_img_resized = resize_right.resize(images_perturb, out_shape=(batch_size,3,256,256))\n",
    "#             tensor_img_resized = tensor_img_resized.to(device)            \n",
    "\n",
    "            batch = {'ori_shape': ((3, 256, 256) for _ in range(batch_size)),\n",
    "                'ratio_pad': None,\n",
    "                'im_file': None,\n",
    "                'img': tensor_img_resized,\n",
    "                'cls': classes,\n",
    "                'bboxes': annotations,\n",
    "                'batch_idx': batch_idx\n",
    "            }\n",
    "            \n",
    "            yolo_out = trainer.model(tensor_img_resized)\n",
    "            \n",
    "            loss_fn = Loss(trainer.model)\n",
    "\n",
    "            loss_adv, _ = loss_fn(yolo_out, batch)\n",
    "            loss_adv *= -1 / batch_size\n",
    "            \n",
    "            loss_gan = net.compute_gen_loss_minimax_modified(discr_model, images_perturb)\n",
    "            loss_hinge = hinge_loss(gen_perturb)\n",
    "            \n",
    "            loss_gen = loss_adv + alpha * loss_gan + beta * loss_hinge\n",
    "            \n",
    "            gen_model.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "            \n",
    "            gen_loss += loss_gen.item()\n",
    "            gen_loss_epoch += loss_gen.item()\n",
    "\n",
    "        loss_gen_list.append(gen_loss / gen_iters)\n",
    "            \n",
    "        if batch_id % 2 == 0:\n",
    "            print('---------------------------------------------------------------------------------------------------------------------')\n",
    "            print(f'epoch: {epoch}; batch: {batch_id}; discr loss: {discr_loss / discr_iters}; gen loss: {gen_loss / gen_iters}')\n",
    "            print(f'adv loss: {loss_adv}; gan loss: {loss_gan}; hinge loss: {loss_hinge}; (from last batch)')\n",
    "            with open(f'GANs_training/{run_name}/training_log.txt', 'a') as f:\n",
    "                f.write('---------------------------------------------------------------------------------------------------------------------')\n",
    "                f.write(f'epoch: {epoch}; batch: {batch_id}; discr loss: {discr_loss / discr_iters}; gen loss: {gen_loss / gen_iters}')\n",
    "                f.write(f'adv loss: {loss_adv}; gan loss: {loss_gan}; hinge loss: {loss_hinge}; (from last batch)')                \n",
    "                f.close()\n",
    "\n",
    "    loss_discr_by_epoch_list.append(discr_loss_epoch / len(dataset_loader) / discr_iters)\n",
    "    loss_gen_by_epoch_list.append(gen_loss_epoch / len(dataset_loader) / gen_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503160db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discr_model.state_dict(), f'GANs_training/{run_name}/discriminator_state_dict')\n",
    "torch.save(gen_model.state_dict(), f'GANs_training/{run_name}/generator_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c003b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss graphs\n",
    "plt.plot(loss_discr_list, label='discr')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_discr_list, label='discr')\n",
    "plt.plot(loss_gen_list, label='gen')\n",
    "plt.legend()\n",
    "plt.savefig(f\"GANs_training/{run_name}/loss_graph.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_discr_by_epoch_list, label='discr')\n",
    "plt.plot(loss_gen_by_epoch_list, label='gen')\n",
    "plt.legend()\n",
    "plt.savefig(f\"GANs_training/{run_name}/loss_graph_by_epoch.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ffa3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen_model.eval().cpu()\n",
    "with torch.no_grad():\n",
    "    x = dataset.image_list[0].unsqueeze(0)\n",
    "    tensor_img = x.repeat(1, 3, 1, 1)\n",
    "    perturb = gen_model(tensor_img)\n",
    "    x_perturbed = tensor_img + perturb\n",
    "\n",
    "tensor_img = tensor_img.squeeze(0).permute(1, 2, 0)\n",
    "perturb = perturb.squeeze(0).permute(1, 2, 0)\n",
    "x_perturbed = x_perturbed.squeeze(0).permute(1, 2, 0)\n",
    "    \n",
    "print('original')\n",
    "plt.imshow(tensor_img)\n",
    "plt.show()\n",
    "print('perturbation')\n",
    "plt.imshow(perturb)\n",
    "plt.show()\n",
    "print('perturbed image')\n",
    "plt.imshow(x_perturbed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90341c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_perturbed_shaped = x_perturbed.permute(2, 0, 1).unsqueeze(0)\n",
    "x_perturbed_reshaped = resize_right.resize(x_perturbed_shaped, out_shape=(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_img_shaped = tensor_img.permute(2, 0, 1).unsqueeze(0)\n",
    "tensor_img_reshaped = resize_right.resize(tensor_img_shaped, out_shape=(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5166362",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.to('cpu')\n",
    "yolo_out = trainer.model(x_perturbed_reshaped)\n",
    "yolo_out2 = trainer.model(tensor_img_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc07b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07efc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(yolo_out[0] - yolo_out2[0]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a34d3",
   "metadata": {},
   "source": [
    "### Summary of Training Procedure\n",
    "\n",
    "*give summary here to save*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22102252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to save notebook before running this cell\n",
    "shutil.copyfile('training.ipynb', f'GANs_training/{run_name}/training.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8452e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torchvision.utils import save_image\n",
    "# gen_model.eval()\n",
    "# for i in range(5):\n",
    "#     with torch.no_grad():\n",
    "#         x = dataset.image_list[i].unsqueeze(0)\n",
    "#         tensor_img = x.repeat(1, 3, 1, 1)\n",
    "#         perturb = gen_model(tensor_img)\n",
    "#         x_perturbed = tensor_img + perturb\n",
    "\n",
    "#     tensor_img = tensor_img.squeeze(0).permute(1, 2, 0)\n",
    "#     perturb = perturb.squeeze(0).permute(1, 2, 0)\n",
    "#     x_perturbed = x_perturbed.squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "#     print('original')\n",
    "#     plt.imshow(tensor_img)\n",
    "#     plt.show()\n",
    "#     print('perturbation')\n",
    "#     plt.imshow(perturb)\n",
    "#     plt.show()\n",
    "#     print('perturbed image')\n",
    "#     plt.imshow(x_perturbed)\n",
    "#     plt.show()\n",
    "    \n",
    "#     save_image(tensor_img.permute(2, 0, 1), f'sample_perturbed_images/original_{i}.png')\n",
    "#     save_image(perturb.permute(2, 0, 1), f'sample_perturbed_images/perturbation_{i}.png')\n",
    "#     save_image(x_perturbed.permute(2, 0, 1), f'sample_perturbed_images/perturbed_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071575e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "gen_model.eval()\n",
    "for i in range(len(dataset.image_list) - 5, len(dataset.image_list)):\n",
    "    with torch.no_grad():\n",
    "        x = dataset.image_list[i].unsqueeze(0)\n",
    "        tensor_img = x.repeat(1, 3, 1, 1)\n",
    "        perturb = gen_model(tensor_img)\n",
    "        x_perturbed = tensor_img + perturb\n",
    "\n",
    "    tensor_img = tensor_img.squeeze(0).permute(1, 2, 0)\n",
    "    perturb = perturb.squeeze(0).permute(1, 2, 0)\n",
    "    x_perturbed = x_perturbed.squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "    print('original')\n",
    "    plt.imshow(tensor_img)\n",
    "    plt.show()\n",
    "    print('perturbation')\n",
    "    plt.imshow(perturb)\n",
    "    plt.show()\n",
    "    print('perturbed image')\n",
    "    plt.imshow(x_perturbed)\n",
    "    plt.show()\n",
    "    \n",
    "    save_image(tensor_img.permute(2, 0, 1), f'sample_perturbed_images/original_small_data_3_{i}.png')\n",
    "    save_image(perturb.permute(2, 0, 1), f'sample_perturbed_images/perturbation_small_data_3_{i}.png')\n",
    "    save_image(x_perturbed.permute(2, 0, 1), f'sample_perturbed_images/perturbed_small_data_3_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe13a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
