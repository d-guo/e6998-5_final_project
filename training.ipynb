{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987e5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "import data\n",
    "import networks as net\n",
    "\n",
    "from networks import Generator, Discriminator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'Blues_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474cf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de20219",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {\n",
    "    'run_name': 'GANs_training_1',\n",
    "    'description': 'sksksksk',\n",
    "    \n",
    "    'image_size': (28, 28 * 5),\n",
    "    \n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 1000000,\n",
    "    \n",
    "    'learning_rate_discr': 0.0001,\n",
    "    'learning_rate_gen': 0.0001,\n",
    "    'discr_iters': 1,\n",
    "    'gen_iters': 4,\n",
    "    \n",
    "    'alpha': 1.0,\n",
    "    'beta': 10.0,\n",
    "    'threshold': 0.5,\n",
    "}\n",
    "\n",
    "# extract all hyperparameters for ease of use\n",
    "run_name = hp_dict['run_name']\n",
    "M, N = hp_dict['image_size']\n",
    "\n",
    "batch_size = hp_dict['batch_size']\n",
    "num_epochs = hp_dict['num_epochs']\n",
    "\n",
    "learning_rate_discr = hp_dict['learning_rate_discr']\n",
    "learning_rate_gen = hp_dict['learning_rate_gen']\n",
    "discr_iters = hp_dict['discr_iters']\n",
    "gen_iters = hp_dict['gen_iters']\n",
    "\n",
    "alpha = hp_dict['alpha']\n",
    "beta = hp_dict['beta']\n",
    "threshold = hp_dict['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7207ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory for this training session\n",
    "try:\n",
    "    os.mkdir(f'GANs_training/{run_name}')\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(f'GANs_training/{run_name}')\n",
    "    os.mkdir(f'GANs_training/{run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe510dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record hyperparameters\n",
    "hp_dict_str = pprint.pformat(hp_dict)\n",
    "\n",
    "with open(f'GANs_training/{run_name}/hp_dict', 'wb+') as f:\n",
    "    pkl.dump(hp_dict, f)\n",
    "\n",
    "with open(f'GANs_training/{run_name}/hp_dict.txt', 'w+') as f:\n",
    "    f.write(hp_dict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9259f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = data.GANs_Dataset()\n",
    "dataset_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38384706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset.image_list[0])\n",
    "plt.imshow(dataset.image_list[0][0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfd0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(gen_perturb):\n",
    "    return torch.sum(torch.clip(torch.norm(gen_perturb, dim=(-1, -2)) - threshold, min=0)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e87982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up models\n",
    "discr_model = Discriminator().double()\n",
    "gen_model = Generator().double()\n",
    "# target_model = # YOLO Model\n",
    "\n",
    "# record model architectures\n",
    "with open(f'GANs_training/{run_name}/discriminator_architecture_summary.txt', 'w+') as f:\n",
    "    f.write(str(discr_model))\n",
    "    \n",
    "with open(f'GANs_training/{run_name}/generator_architecture_summary.txt', 'w+') as f:\n",
    "    f.write(str(gen_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff9e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.86 ðŸš€ Python-3.9.16 torch-2.0.0 CPU\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=predict, model=model_results/model_digit_data_ver1_256/weights/best.pt, data=detection_data.yaml, epochs=100, patience=50, batch=32, imgsz=256, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=None, exist_ok=False, pretrained=False, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.Detect                [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.yolo.v8.detect.train import DetectionTrainer\n",
    "from ultralytics.yolo.data.dataloaders.v5loader import create_dataloader\n",
    "from ultralytics.yolo.v8.detect.train import Loss\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import resize_right\n",
    "import numpy as np\n",
    "\n",
    "# load detection trainer using the weights best.pt\n",
    "# best.pt is the custom model trained on YOLO_data_je\n",
    "trainer = DetectionTrainer('args_digits_data_ver1.yaml')\n",
    "trainer.setup_model()\n",
    "trainer.model.double()\n",
    "trainer.set_model_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ab53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track metrics\n",
    "loss_discr_list = []\n",
    "loss_gen_list = []\n",
    "\n",
    "loss_discr_by_epoch_list = []\n",
    "loss_gen_by_epoch_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe3a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up or refresh optimizers\n",
    "optimizer_discr = torch.optim.Adam(discr_model.parameters(), lr=learning_rate_discr, amsgrad=True)\n",
    "optimizer_gen = torch.optim.Adam(gen_model.parameters(), lr=learning_rate_gen, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef1ba570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 5 but got size 160 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m yolo_out \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodel(tensor_img_resized)\n\u001b[1;32m     46\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m Loss(trainer\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 47\u001b[0m loss_adv, _ \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m loss_adv \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m loss_gan \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mcompute_gen_loss_minimax_modified(discr_model, images_perturb)\n",
      "File \u001b[0;32m~/miniforge3/envs/hpml_env/lib/python3.9/site-packages/ultralytics/yolo/v8/detect/train.py:181\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# targets\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbboxes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_size, scale_tensor\u001b[38;5;241m=\u001b[39mimgsz[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m    183\u001b[0m gt_labels, gt_bboxes \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# cls, xyxy\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 5 but got size 160 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# train\n",
    "discr_model.train()\n",
    "gen_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    discr_loss_epoch, gen_loss_epoch = 0, 0\n",
    "    for batch_id, (images, classes, annotations) in enumerate(dataset_loader):\n",
    "        images = images.double()\n",
    "        classes = classes.double()\n",
    "        annotations = annotations.double()\n",
    "\n",
    "        tensor_img = images.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        discr_loss, gen_loss = 0, 0\n",
    "        \n",
    "        for discr_i in range(discr_iters):\n",
    "            gen_perturb = gen_model(tensor_img)\n",
    "            images_perturb = tensor_img + gen_perturb\n",
    "            \n",
    "            loss_discr = net.compute_discr_loss_minimax(discr_model, tensor_img, images_perturb)\n",
    "            \n",
    "            optimizer_discr.zero_grad()\n",
    "            loss_discr.backward()\n",
    "            optimizer_discr.step()\n",
    "            \n",
    "            discr_loss += loss_discr.item()\n",
    "            discr_loss_epoch += loss_discr.item()\n",
    "\n",
    "        loss_discr_list.append(discr_loss / discr_iters)\n",
    "\n",
    "        for gen_i in range(gen_iters):\n",
    "            gen_perturb = gen_model(tensor_img)\n",
    "            images_perturb = tensor_img + gen_perturb\n",
    "            \n",
    "            tensor_img_resized = resize_right.resize(images_perturb, out_shape=(1,3,256,256))\n",
    "            batch = {'ori_shape': ((3, 28, 140),),\n",
    "                'ratio_pad': None,\n",
    "                'im_file': None,\n",
    "                'img': tensor_img_resized,\n",
    "                'cls': classes.squeeze(0),\n",
    "                'bboxes': annotations.squeeze(0),\n",
    "                'batch_idx': torch.zeros(classes.shape[1])\n",
    "            }\n",
    "            \n",
    "            yolo_out = trainer.model(tensor_img_resized)\n",
    "            loss_fn = Loss(trainer.model)\n",
    "            loss_adv, _ = loss_fn(yolo_out, batch)\n",
    "            loss_adv *= -1\n",
    "            \n",
    "            loss_gan = net.compute_gen_loss_minimax_modified(discr_model, images_perturb)\n",
    "            \n",
    "            loss_hinge = hinge_loss(gen_perturb)\n",
    "            \n",
    "            loss_gen = loss_adv + alpha * loss_gan + beta * loss_hinge\n",
    "            \n",
    "            gen_model.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "            \n",
    "            gen_loss += loss_gen.item()\n",
    "            gen_loss_epoch += loss_gen.item()\n",
    "\n",
    "        loss_gen_list.append(gen_loss / gen_iters)\n",
    "            \n",
    "        if batch_id % 10 == 0:\n",
    "            print('---------------------------------------------------------------------------------------------------------------------')\n",
    "            print(f'epoch: {epoch}; batch: {batch_id}; discr loss: {discr_loss / discr_iters}; gen loss: {gen_loss / gen_iters}')\n",
    "            print(f'adv loss: {loss_adv}; gan loss: {loss_gan}; hinge loss: {loss_hinge}; (from last batch)')\n",
    "            \n",
    "    loss_discr_by_epoch_list.append(discr_loss_epoch / len(dataset_loader) / discr_iters)\n",
    "    loss_gen_by_epoch_list.append(gen_loss_epoch / len(dataset_loader) / gen_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss graphs\n",
    "plt.plot(loss_discr_list, label='discr')\n",
    "plt.plot(loss_gen_list, label='gen')\n",
    "plt.legend()\n",
    "plt.savefig(f\"GANs_training/{run_name}/loss_graph.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_discr_by_epoch_list, label='discr')\n",
    "plt.plot(loss_gen_by_epoch_list, label='gen')\n",
    "plt.legend()\n",
    "plt.savefig(f\"GANs_training/{run_name}/loss_graph_by_epoch.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discr_model.state_dict(), f'GANs_training/{run_name}/discriminator_state_dict')\n",
    "torch.save(gen_model.state_dict(), f'GANs_training/{run_name}/generator_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ffa3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen_model.eval()\n",
    "with torch.no_grad():\n",
    "    x = dataset.image_list[0].unsqueeze(0)\n",
    "    tensor_img = x.repeat(1, 3, 1, 1)\n",
    "    perturb = gen_model(tensor_img)\n",
    "    x_perturbed = tensor_img + perturb\n",
    "\n",
    "tensor_img = tensor_img.squeeze(0).permute(1, 2, 0)\n",
    "perturb = perturb.squeeze(0).permute(1, 2, 0)\n",
    "x_perturbed = x_perturbed.squeeze(0).permute(1, 2, 0)\n",
    "    \n",
    "print('original')\n",
    "plt.imshow(tensor_img)\n",
    "plt.show()\n",
    "print('perturbation')\n",
    "plt.imshow(perturb)\n",
    "plt.show()\n",
    "print('perturbed image')\n",
    "plt.imshow(x_perturbed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90341c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_perturbed_shaped = x_perturbed.permute(2, 0, 1).unsqueeze(0)\n",
    "x_perturbed_reshaped = resize_right.resize(x_perturbed_shaped, out_shape=(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_img_shaped = tensor_img.permute(2, 0, 1).unsqueeze(0)\n",
    "tensor_img_reshaped = resize_right.resize(tensor_img_shaped, out_shape=(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5166362",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out = trainer.model(x_perturbed_reshaped)\n",
    "yolo_out2 = trainer.model(tensor_img_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc07b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07efc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(yolo_out[0] - yolo_out2[0]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a34d3",
   "metadata": {},
   "source": [
    "### Summary of Training Procedure\n",
    "\n",
    "*give summary here to save*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22102252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to save notebook before running this cell\n",
    "shutil.copyfile('training.ipynb', f'GANs_training/{run_name}/training.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8452e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "gen_model.eval()\n",
    "for i in range(5):\n",
    "    with torch.no_grad():\n",
    "        x = dataset.image_list[i].unsqueeze(0)\n",
    "        tensor_img = x.repeat(1, 3, 1, 1)\n",
    "        perturb = gen_model(tensor_img)\n",
    "        x_perturbed = tensor_img + perturb\n",
    "\n",
    "    tensor_img = tensor_img.squeeze(0).permute(1, 2, 0)\n",
    "    perturb = perturb.squeeze(0).permute(1, 2, 0)\n",
    "    x_perturbed = x_perturbed.squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "    print('original')\n",
    "    plt.imshow(tensor_img)\n",
    "    plt.show()\n",
    "    print('perturbation')\n",
    "    plt.imshow(perturb)\n",
    "    plt.show()\n",
    "    print('perturbed image')\n",
    "    plt.imshow(x_perturbed)\n",
    "    plt.show()\n",
    "    \n",
    "    save_image(tensor_img.permute(2, 0, 1), f'sample_perturbed_images/original_{i}.png')\n",
    "    save_image(perturb.permute(2, 0, 1), f'sample_perturbed_images/perturbation_{i}.png')\n",
    "    save_image(x_perturbed.permute(2, 0, 1), f'sample_perturbed_images/perturbed_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071575e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "gen_model.eval()\n",
    "for i in range(len(dataset.image_list) - 5, len(dataset.image_list)):\n",
    "    with torch.no_grad():\n",
    "        x = dataset.image_list[i].unsqueeze(0)\n",
    "        tensor_img = x.repeat(1, 3, 1, 1)\n",
    "        perturb = gen_model(tensor_img)\n",
    "        x_perturbed = tensor_img + perturb\n",
    "\n",
    "    tensor_img = tensor_img.squeeze(0).permute(1, 2, 0)\n",
    "    perturb = perturb.squeeze(0).permute(1, 2, 0)\n",
    "    x_perturbed = x_perturbed.squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "    print('original')\n",
    "    plt.imshow(tensor_img)\n",
    "    plt.show()\n",
    "    print('perturbation')\n",
    "    plt.imshow(perturb)\n",
    "    plt.show()\n",
    "    print('perturbed image')\n",
    "    plt.imshow(x_perturbed)\n",
    "    plt.show()\n",
    "    \n",
    "    save_image(tensor_img.permute(2, 0, 1), f'sample_perturbed_images/original_{i}.png')\n",
    "    save_image(perturb.permute(2, 0, 1), f'sample_perturbed_images/perturbation_{i}.png')\n",
    "    save_image(x_perturbed.permute(2, 0, 1), f'sample_perturbed_images/perturbed_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccf5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b3941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
